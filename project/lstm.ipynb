{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "TEST_SIZE=2048\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 1000\n",
    "maxSeqLength = 250\n",
    "numDimensions = 300\n",
    "\n",
    "# load the GLOVE arrays, smaller than w2v google, hopefully no perfomance drop\n",
    "wordsList = np.load('data/wordsList.npy')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('utf-8') for word in wordsList] \n",
    "wordVectors = np.load('data/wordVectors.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getTrainData():\n",
    "    train_files = [f for f in os.listdir(\"data/shuffled\") if f.startswith(\"reviews\")]\n",
    "    \n",
    "    frames = [np.load(\"data/shuffled/\" + f) for f in train_files]\n",
    "    labels = [np.load(\"data/shuffled/\" + f.replace(\"reviews\", \"labels\")) for f in train_files]\n",
    "    \n",
    "    X = np.vstack(frames)\n",
    "    y = np.vstack(labels)\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "train_data, train_labels = getTrainData()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape (81920, 250)\n",
      "train labels shape (81920, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape\", train_data.shape)\n",
    "print(\"train labels shape\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_bow_vector(rev):\n",
    "    global wordsList\n",
    "    rev = re.sub('[^-a-zA-Z0-9_ -]+', '', rev)\n",
    "    split_words = rev.split()\n",
    "    \n",
    "    firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "    indexCounter = 0\n",
    "    for word in split_words[0:250]:\n",
    "        try: \n",
    "            firstFile[indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "    return firstFile\n",
    "\n",
    "\n",
    "def getNextFrame():\n",
    "    file_name = getNextFile()\n",
    "    frame = pd.read_csv(file_name, encoding='utf-8')\n",
    "    frame = frame[(frame.clean_text.notnull()) & (frame.clean_text.str.len() > 100)]\n",
    "    print(\"new frame loaded\", file_name, frame.shape)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def getNextFile():\n",
    "    global file_count\n",
    "    global good_files\n",
    "    file_count += 1\n",
    "    return good_files[file_count]\n",
    "\n",
    "def getTrainBatch():\n",
    "    global train_data\n",
    "    global train_labels\n",
    "        \n",
    "    ix = np.random.randint(train_data.shape[0], size=BATCH_SIZE)\n",
    "    return train_data[ix,], train_labels[ix, ]\n",
    "\n",
    "\n",
    "def getTestBatch(size=None):\n",
    "    \n",
    "\n",
    "    arr = np.load(\"data/shuffled/test_reviews_shuffled_0.npy\")\n",
    "    labels = np.load(\"data/shuffled/test_labels_shuffled_0.npy\")\n",
    "    \n",
    "    if size is not None:\n",
    "        ix = np.random.randint(arr.shape[0], size=size)\n",
    "        arr = arr[ix,]\n",
    "        labels = labels[ix,]\n",
    "    \n",
    "    return arr, labels\n",
    "\n",
    "\n",
    "def one_hot_label(label):\n",
    "    if label==0:\n",
    "        return np.array([1,0])\n",
    "    else:\n",
    "        return np.array([0,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wordVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048 (2048, 2)\n",
      "(2048, 250)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data, test_labels = getTestBatch(size=TEST_SIZE)\n",
    "print(len(test_labels), test_labels.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 2)\n",
      "(2048, 250)\n"
     ]
    }
   ],
   "source": [
    "print(test_labels.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [None, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [None, maxSeqLength])\n",
    "\n",
    "\n",
    "embedding = tf.get_variable(name=\"word_embedding\", shape=wordVectors.shape, initializer=tf.constant_initializer(wordVectors), trainable=False)\n",
    "\n",
    "\n",
    "#data = tf.Variable(tf.zeros([None, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(embedding,input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_step = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "train accuracy 0.218750\n",
      "****************\n",
      "test accuracy:   0.856445\n",
      "step 10\n",
      "train accuracy 0.835938\n",
      "step 20\n",
      "train accuracy 0.906250\n",
      "step 30\n",
      "train accuracy 0.796875\n",
      "step 40\n",
      "train accuracy 0.867188\n",
      "step 50\n",
      "train accuracy 0.851562\n",
      "****************\n",
      "test accuracy:   0.879883\n",
      "step 60\n",
      "train accuracy 0.882812\n",
      "step 70\n",
      "train accuracy 0.890625\n",
      "step 80\n",
      "train accuracy 0.914062\n",
      "step 90\n",
      "train accuracy 0.789062\n",
      "step 100\n",
      "train accuracy 0.859375\n",
      "****************\n",
      "test accuracy:   0.880859\n",
      "step 110\n",
      "train accuracy 0.851562\n",
      "step 120\n",
      "train accuracy 0.890625\n",
      "step 130\n",
      "train accuracy 0.843750\n",
      "step 140\n",
      "train accuracy 0.882812\n",
      "step 150\n",
      "train accuracy 0.867188\n",
      "****************\n",
      "test accuracy:   0.880371\n",
      "step 160\n",
      "train accuracy 0.859375\n",
      "step 170\n",
      "train accuracy 0.882812\n",
      "step 180\n",
      "train accuracy 0.843750\n",
      "step 190\n",
      "train accuracy 0.867188\n",
      "step 200\n",
      "train accuracy 0.898438\n",
      "****************\n",
      "test accuracy:   0.880859\n",
      "step 210\n",
      "train accuracy 0.875000\n",
      "step 220\n",
      "train accuracy 0.898438\n",
      "step 230\n",
      "train accuracy 0.835938\n",
      "step 240\n",
      "train accuracy 0.875000\n",
      "step 250\n",
      "train accuracy 0.890625\n",
      "****************\n",
      "test accuracy:   0.880371\n",
      "step 260\n",
      "train accuracy 0.890625\n",
      "step 270\n",
      "train accuracy 0.890625\n",
      "step 280\n",
      "train accuracy 0.882812\n",
      "step 290\n",
      "train accuracy 0.843750\n",
      "step 300\n",
      "train accuracy 0.867188\n",
      "****************\n",
      "test accuracy:   0.879883\n",
      "step 310\n",
      "train accuracy 0.914062\n",
      "step 320\n",
      "train accuracy 0.828125\n",
      "step 330\n",
      "train accuracy 0.867188\n",
      "step 340\n",
      "train accuracy 0.906250\n",
      "step 350\n",
      "train accuracy 0.867188\n",
      "****************\n",
      "test accuracy:   0.879395\n",
      "step 360\n",
      "train accuracy 0.882812\n",
      "step 370\n",
      "train accuracy 0.867188\n",
      "step 380\n",
      "train accuracy 0.867188\n",
      "step 390\n",
      "train accuracy 0.890625\n",
      "step 400\n",
      "train accuracy 0.875000\n",
      "****************\n",
      "test accuracy:   0.880859\n",
      "step 410\n",
      "train accuracy 0.859375\n",
      "step 420\n",
      "train accuracy 0.882812\n",
      "step 430\n",
      "train accuracy 0.906250\n",
      "step 440\n",
      "train accuracy 0.890625\n",
      "step 450\n",
      "train accuracy 0.890625\n",
      "****************\n",
      "test accuracy:   0.880859\n",
      "step 460\n",
      "train accuracy 0.828125\n",
      "step 470\n",
      "train accuracy 0.890625\n",
      "step 480\n",
      "train accuracy 0.875000\n",
      "step 490\n",
      "train accuracy 0.867188\n",
      "step 500\n",
      "train accuracy 0.835938\n",
      "****************\n",
      "test accuracy:   0.877441\n",
      "step 510\n",
      "train accuracy 0.851562\n",
      "step 520\n",
      "train accuracy 0.890625\n",
      "step 530\n",
      "train accuracy 0.851562\n",
      "step 540\n",
      "train accuracy 0.890625\n",
      "step 550\n",
      "train accuracy 0.914062\n",
      "****************\n",
      "test accuracy:   0.880371\n",
      "step 560\n",
      "train accuracy 0.875000\n",
      "step 570\n",
      "train accuracy 0.843750\n",
      "step 580\n",
      "train accuracy 0.914062\n",
      "step 590\n",
      "train accuracy 0.867188\n",
      "step 600\n",
      "train accuracy 0.882812\n",
      "****************\n",
      "test accuracy:   0.880859\n",
      "step 610\n",
      "train accuracy 0.921875\n",
      "step 620\n",
      "train accuracy 0.890625\n",
      "step 630\n",
      "train accuracy 0.828125\n",
      "step 640\n",
      "train accuracy 0.921875\n",
      "step 650\n",
      "train accuracy 0.867188\n",
      "****************\n",
      "test accuracy:   0.880859\n",
      "step 660\n",
      "train accuracy 0.859375\n",
      "step 670\n",
      "train accuracy 0.898438\n",
      "step 680\n",
      "train accuracy 0.875000\n",
      "step 690\n",
      "train accuracy 0.937500\n",
      "step 700\n",
      "train accuracy 0.867188\n",
      "****************\n",
      "test accuracy:   0.880371\n",
      "step 710\n",
      "train accuracy 0.859375\n",
      "step 720\n",
      "train accuracy 0.898438\n",
      "step 730\n",
      "train accuracy 0.812500\n",
      "step 740\n",
      "train accuracy 0.859375\n",
      "step 750\n",
      "train accuracy 0.921875\n",
      "****************\n",
      "test accuracy:   0.880371\n",
      "step 760\n",
      "train accuracy 0.843750\n",
      "step 770\n",
      "train accuracy 0.859375\n",
      "step 780\n",
      "train accuracy 0.867188\n",
      "step 790\n",
      "train accuracy 0.890625\n",
      "step 800\n",
      "train accuracy 0.859375\n",
      "****************\n",
      "test accuracy:   0.879395\n",
      "step 810\n",
      "train accuracy 0.851562\n",
      "step 820\n",
      "train accuracy 0.851562\n",
      "step 830\n",
      "train accuracy 0.898438\n",
      "step 840\n",
      "train accuracy 0.867188\n",
      "step 850\n",
      "train accuracy 0.890625\n",
      "****************\n",
      "test accuracy:   0.880371\n",
      "step 860\n",
      "train accuracy 0.867188\n",
      "step 870\n",
      "train accuracy 0.914062\n",
      "step 880\n",
      "train accuracy 0.882812\n",
      "step 890\n",
      "train accuracy 0.890625\n",
      "step 900\n",
      "train accuracy 0.875000\n",
      "****************\n",
      "test accuracy:   0.880859\n",
      "step 910\n",
      "train accuracy 0.812500\n",
      "step 920\n",
      "train accuracy 0.882812\n",
      "step 930\n",
      "train accuracy 0.890625\n",
      "step 940\n",
      "train accuracy 0.906250\n",
      "step 950\n",
      "train accuracy 0.882812\n",
      "****************\n",
      "test accuracy:   0.880859\n",
      "step 960\n",
      "train accuracy 0.882812\n",
      "step 970\n",
      "train accuracy 0.898438\n",
      "step 980\n",
      "train accuracy 0.875000\n",
      "step 990\n",
      "train accuracy 0.867188\n",
      "saved to models/final_lstm.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    _, summary, acc = sess.run([ train_step, merged, accuracy], {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "    #Save the network every 10,000 training iterations\n",
    "    if i % 10 == 0:\n",
    "        writer.add_summary(summary, i)\n",
    "        print(\"step %d\" % i)\n",
    "        print(\"train accuracy %f\" % acc)\n",
    "    \n",
    "    if i % 50 == 0 :\n",
    "        print(\"****************\")\n",
    "        print(\"test accuracy:  % f\" % accuracy.eval({input_data:test_data, labels: test_labels}))\n",
    "        \n",
    "        \n",
    "save_path = \"models/final_lstm.ckpt\"\n",
    "print(\"saved to %s\" % save_path)\n",
    "\n",
    "save_path = saver.save(sess, save_path, global_step=10000)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
