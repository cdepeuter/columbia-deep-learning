{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_reviews_Automotive_5_word_vecs.npy\r\n",
      "0_reviews_Beauty_5_review_embeds.npy\r\n",
      "0_reviews_Beauty_5_word_vecs.npy\r\n",
      "0_reviews_Digital_Music_5_word_vecs.npy\r\n",
      "0_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "10_reviews_Beauty_5_review_embeds.npy\r\n",
      "10_reviews_Beauty_5_word_vecs.npy\r\n",
      "10_reviews_Digital_Music_5_word_vecs.npy\r\n",
      "10_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "11_reviews_Beauty_5_review_embeds.npy\r\n",
      "11_reviews_Beauty_5_word_vecs.npy\r\n",
      "11_reviews_Digital_Music_5_word_vecs.npy\r\n",
      "11_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "12_reviews_Beauty_5_review_embeds.npy\r\n",
      "12_reviews_Beauty_5_word_vecs.npy\r\n",
      "12_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "13_reviews_Beauty_5_review_embeds.npy\r\n",
      "13_reviews_Beauty_5_word_vecs.npy\r\n",
      "13_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "14_reviews_Beauty_5_review_embeds.npy\r\n",
      "14_reviews_Beauty_5_word_vecs.npy\r\n",
      "14_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "15_reviews_Beauty_5_review_embeds.npy\r\n",
      "15_reviews_Beauty_5_word_vecs.npy\r\n",
      "15_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "16_reviews_Beauty_5_review_embeds.npy\r\n",
      "16_reviews_Beauty_5_word_vecs.npy\r\n",
      "16_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "17_reviews_Beauty_5_review_embeds.npy\r\n",
      "17_reviews_Beauty_5_word_vecs.npy\r\n",
      "17_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "18_reviews_Beauty_5_review_embeds.npy\r\n",
      "18_reviews_Beauty_5_word_vecs.npy\r\n",
      "18_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "19_reviews_Beauty_5_word_vecs.npy\r\n",
      "19_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "1_reviews_Automotive_5_word_vecs.npy\r\n",
      "1_reviews_Beauty_5_review_embeds.npy\r\n",
      "1_reviews_Beauty_5_word_vecs.npy\r\n",
      "1_reviews_Digital_Music_5_word_vecs.npy\r\n",
      "1_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "20_reviews_Beauty_5_word_vecs.npy\r\n",
      "21_reviews_Beauty_5_word_vecs.npy\r\n",
      "22_reviews_Beauty_5_word_vecs.npy\r\n",
      "23_reviews_Beauty_5_word_vecs.npy\r\n",
      "24_reviews_Beauty_5_word_vecs.npy\r\n",
      "25_reviews_Beauty_5_word_vecs.npy\r\n",
      "26_reviews_Beauty_5_word_vecs.npy\r\n",
      "27_reviews_Beauty_5_word_vecs.npy\r\n",
      "28_reviews_Beauty_5_word_vecs.npy\r\n",
      "29_reviews_Beauty_5_word_vecs.npy\r\n",
      "2_reviews_Automotive_5_word_vecs.npy\r",
      "\r\n",
      "2_reviews_Beauty_5_review_embeds.npy\r\n",
      "2_reviews_Beauty_5_word_vecs.npy\r\n",
      "2_reviews_Digital_Music_5_word_vecs.npy\r\n",
      "2_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "30_reviews_Beauty_5_word_vecs.npy\r\n",
      "31_reviews_Beauty_5_word_vecs.npy\r\n",
      "32_reviews_Beauty_5_word_vecs.npy\r\n",
      "33_reviews_Beauty_5_word_vecs.npy\r\n",
      "34_reviews_Beauty_5_word_vecs.npy\r\n",
      "35_reviews_Beauty_5_word_vecs.npy\r\n",
      "36_reviews_Beauty_5_word_vecs.npy\r\n",
      "37_reviews_Beauty_5_word_vecs.npy\r\n",
      "38_reviews_Beauty_5_word_vecs.npy\r\n",
      "3_reviews_Automotive_5_word_vecs.npy\r\n",
      "3_reviews_Beauty_5_review_embeds.npy\r\n",
      "3_reviews_Beauty_5_word_vecs.npy\r\n",
      "3_reviews_Digital_Music_5_word_vecs.npy\r\n",
      "3_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "4_reviews_Automotive_5_word_vecs.npy\r\n",
      "4_reviews_Beauty_5_review_embeds.npy\r\n",
      "4_reviews_Beauty_5_word_vecs.npy\r\n",
      "4_reviews_Digital_Music_5_word_vecs.npy\r\n",
      "4_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "5_reviews_Automotive_5_word_vecs.npy\r\n",
      "5_reviews_Beauty_5_review_embeds.npy\r\n",
      "5_reviews_Beauty_5_word_vecs.npy\r\n",
      "5_reviews_Digital_Music_5_word_vecs.npy\r\n",
      "5_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "6_reviews_Automotive_5_word_vecs.npy\r\n",
      "6_reviews_Beauty_5_review_embeds.npy\r\n",
      "6_reviews_Beauty_5_word_vecs.npy\r\n",
      "6_reviews_Digital_Music_5_word_vecs.npy\r\n",
      "6_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "7_reviews_Automotive_5_word_vecs.npy\r\n",
      "7_reviews_Beauty_5_review_embeds.npy\r\n",
      "7_reviews_Beauty_5_word_vecs.npy\r\n",
      "7_reviews_Digital_Music_5_word_vecs.npy\r\n",
      "7_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "8_reviews_Automotive_5_word_vecs.npy\r\n",
      "8_reviews_Beauty_5_review_embeds.npy\r\n",
      "8_reviews_Beauty_5_word_vecs.npy\r\n",
      "8_reviews_Digital_Music_5_word_vecs.npy\r\n",
      "8_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "9_reviews_Automotive_5_word_vecs.npy\r\n",
      "9_reviews_Beauty_5_review_embeds.npy\r\n",
      "9_reviews_Beauty_5_word_vecs.npy\r\n",
      "9_reviews_Digital_Music_5_word_vecs.npy\r\n",
      "9_reviews_Musical_Instruments_5_word_vecs.npy\r\n",
      "cleaned_data.csv\r\n",
      "reviews_Amazon_Instant_Video_5.csv\r\n",
      "reviews_Amazon_Instant_Video_5.json\r\n",
      "reviews_Amazon_Instant_Video_5.json.gz\r\n",
      "reviews_Amazon_Instant_Video_5_word_vecs.csv\r\n",
      "reviews_Amazon_Instant_Video_5_word_vecs.npy\r\n",
      "reviews_Apps_for_Android_5.csv\r\n",
      "reviews_Apps_for_Android_5.json\r\n",
      "reviews_Apps_for_Android_5.json.gz\r\n",
      "reviews_Automotive_5 2.json\r\n",
      "reviews_Automotive_5.csv\r\n",
      "reviews_Automotive_5.json\r\n",
      "reviews_Automotive_5.json.gz\r\n",
      "reviews_Baby_5.csv\r\n",
      "reviews_Baby_5.json\r\n",
      "reviews_Baby_5.json.gz\r\n",
      "reviews_Beauty_5.csv\r\n",
      "reviews_Beauty_5.json\r\n",
      "reviews_Beauty_5.json.gz\r\n",
      "reviews_Beauty_5_review_embeds.npy\r\n",
      "reviews_CDs_and_Vinyl_5.json\r\n",
      "reviews_CDs_and_Vinyl_5.json.gz\r\n",
      "reviews_Cell_Phones_and_Accessories_5.csv\r\n",
      "reviews_Cell_Phones_and_Accessories_5.json\r\n",
      "reviews_Cell_Phones_and_Accessories_5.json.gz\r\n",
      "reviews_Clothing_Shoes_and_Jewelry_5.csv\r\n",
      "reviews_Clothing_Shoes_and_Jewelry_5.json\r\n",
      "reviews_Clothing_Shoes_and_Jewelry_5.json.gz\r\n",
      "reviews_Digital_Music_5.csv\r\n",
      "reviews_Digital_Music_5.json\r\n",
      "reviews_Digital_Music_5.json.gz\r\n",
      "reviews_Electronics_5.json\r\n",
      "reviews_Electronics_5.json.gz\r\n",
      "reviews_Electronics_5_0.csv\r\n",
      "reviews_Electronics_5_1.csv\r\n",
      "reviews_Electronics_5_2.csv\r\n",
      "reviews_Electronics_5_3.csv\r\n",
      "reviews_Electronics_5_4.csv\r\n",
      "reviews_Electronics_5_5.csv\r\n",
      "reviews_Electronics_5_6.csv\r\n",
      "reviews_Grocery_and_Gourmet_Food_5.csv\r\n",
      "reviews_Grocery_and_Gourmet_Food_5.json\r\n",
      "reviews_Grocery_and_Gourmet_Food_5.json.gz\r\n",
      "reviews_Health_and_Personal_Care_5.csv\r\n",
      "reviews_Health_and_Personal_Care_5.json\r\n",
      "reviews_Health_and_Personal_Care_5.json.gz\r\n",
      "reviews_Home_and_Kitchen_5.json\r\n",
      "reviews_Home_and_Kitchen_5.json.gz\r\n",
      "reviews_Home_and_Kitchen_5_0.csv\r\n",
      "reviews_Home_and_Kitchen_5_1.csv\r\n",
      "reviews_Kindle_Store_5.json\r\n",
      "reviews_Kindle_Store_5.json.gz\r\n",
      "reviews_Movies_and_TV_5.json\r\n",
      "reviews_Movies_and_TV_5.json.gz\r\n",
      "reviews_Musical_Instruments_5.csv\r\n",
      "reviews_Musical_Instruments_5.json\r\n",
      "reviews_Musical_Instruments_5.json.gz\r\n",
      "reviews_Office_Products_5.csv\r\n",
      "reviews_Office_Products_5.json\r\n",
      "reviews_Office_Products_5.json.gz\r\n",
      "reviews_Patio_Lawn_and_Garden_5.csv\r\n",
      "reviews_Patio_Lawn_and_Garden_5.json\r\n",
      "reviews_Patio_Lawn_and_Garden_5.json.gz\r\n",
      "reviews_Pet_Supplies_5.csv\r\n",
      "reviews_Pet_Supplies_5.json\r\n",
      "reviews_Pet_Supplies_5.json.gz\r\n",
      "reviews_Sports_and_Outdoors_5.csv\r\n",
      "reviews_Sports_and_Outdoors_5.json\r\n",
      "reviews_Sports_and_Outdoors_5.json.gz\r\n",
      "reviews_Tools_and_Home_Improvement_5.csv\r\n",
      "reviews_Tools_and_Home_Improvement_5.json\r\n",
      "reviews_Tools_and_Home_Improvement_5.json.gz\r\n",
      "reviews_Video_Games_5.json\r\n",
      "reviews_Video_Games_5.json.gz\r\n",
      "\u001b[34mshuffled\u001b[m\u001b[m/\r\n",
      "\u001b[34mtoo_big\u001b[m\u001b[m/\r\n",
      "\u001b[34mvecs\u001b[m\u001b[m/\r\n",
      "w2v_vectors.npy\r\n",
      "wordVectors.npy\r\n",
      "wordsList.npy\r\n"
     ]
    }
   ],
   "source": [
    "ls data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reviews_Amazon_Instant_Video_5.csv',\n",
       " 'reviews_Apps_for_Android_5.csv',\n",
       " 'reviews_Automotive_5.csv',\n",
       " 'reviews_Baby_5.csv',\n",
       " 'reviews_Beauty_5.csv',\n",
       " 'reviews_Cell_Phones_and_Accessories_5.csv',\n",
       " 'reviews_Clothing_Shoes_and_Jewelry_5.csv',\n",
       " 'reviews_Digital_Music_5.csv',\n",
       " 'reviews_Electronics_5_0.csv',\n",
       " 'reviews_Electronics_5_1.csv',\n",
       " 'reviews_Electronics_5_2.csv',\n",
       " 'reviews_Electronics_5_3.csv',\n",
       " 'reviews_Electronics_5_4.csv',\n",
       " 'reviews_Electronics_5_5.csv',\n",
       " 'reviews_Electronics_5_6.csv',\n",
       " 'reviews_Grocery_and_Gourmet_Food_5.csv',\n",
       " 'reviews_Health_and_Personal_Care_5.csv',\n",
       " 'reviews_Home_and_Kitchen_5_0.csv',\n",
       " 'reviews_Home_and_Kitchen_5_1.csv',\n",
       " 'reviews_Musical_Instruments_5.csv',\n",
       " 'reviews_Office_Products_5.csv',\n",
       " 'reviews_Patio_Lawn_and_Garden_5.csv',\n",
       " 'reviews_Pet_Supplies_5.csv',\n",
       " 'reviews_Sports_and_Outdoors_5.csv',\n",
       " 'reviews_Tools_and_Home_Improvement_5.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_files = [ f for f in os.listdir(\"data\") if f.endswith(\".csv\") and f.startswith(\"reviews_\") and \"word_vecs\" not in f]\n",
    "cleaned_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_label(label):\n",
    "    if label==0:\n",
    "        return np.array([1,0])\n",
    "    else:\n",
    "        return np.array([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# batchSize = 1024\n",
    "# lstmUnits = 64\n",
    "# numClasses = 2\n",
    "# iterations = 100\n",
    "# maxSeqLength = 50\n",
    "\n",
    "\n",
    "\n",
    "def review_to_bow_vector(rev):\n",
    "    rev = re.sub('[^-a-zA-Z0-9_ -]+', '', rev)\n",
    "    split_words = rev.split()\n",
    "    \n",
    "    firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "    indexCounter = 0\n",
    "    for word in split_words[0:250]:\n",
    "        try: \n",
    "            firstFile[indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "    return firstFile\n",
    "\n",
    "\n",
    "# load the GLOVE arrays, smaller than w2v google, hopefully no perfomance drop\n",
    "wordsList = np.load('data/wordsList.npy')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('utf-8') for word in wordsList] \n",
    "wordVectors = np.load('data/wordVectors.npy')\n",
    "\n",
    "#good_files = [\"data/\"+f for f in os.listdir(\"data\") if \".csv\" in f]\n",
    "good_files = [\"data/reviews_Musical_Instruments_5.csv\", \"data/reviews_Automotive_5.csv\", \"data/reviews_Amazon_Instant_Video_5.csv\"]\n",
    "file_count = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews_Amazon_Instant_Video_5.csv\n",
      "frame shape (32939, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (31427, 2)\n",
      "reviews_Apps_for_Android_5.csv\n",
      "frame shape (667816, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (592482, 2)\n",
      "reviews_Automotive_5.csv\n",
      "frame shape (19043, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (18299, 2)\n",
      "reviews_Baby_5.csv\n",
      "frame shape (143537, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (139679, 2)\n",
      "reviews_Beauty_5.csv\n",
      "frame shape (176254, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (169808, 2)\n",
      "reviews_Cell_Phones_and_Accessories_5.csv\n",
      "frame shape (173000, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (162479, 2)\n",
      "reviews_Clothing_Shoes_and_Jewelry_5.csv\n",
      "frame shape (248252, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (234449, 2)\n",
      "reviews_Digital_Music_5.csv\n",
      "frame shape (57917, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (56608, 2)\n",
      "reviews_Electronics_5_0.csv\n",
      "frame shape (200000, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (190932, 2)\n",
      "reviews_Electronics_5_1.csv\n",
      "frame shape (200000, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (191503, 2)\n",
      "reviews_Electronics_5_2.csv\n",
      "frame shape (200000, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (192792, 2)\n",
      "reviews_Electronics_5_3.csv\n",
      "frame shape (200000, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (193181, 2)\n",
      "reviews_Electronics_5_4.csv\n",
      "frame shape (200000, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (193582, 2)\n",
      "reviews_Electronics_5_5.csv\n",
      "frame shape (200000, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (193336, 2)\n",
      "reviews_Electronics_5_6.csv\n",
      "frame shape (200000, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (192400, 2)\n",
      "reviews_Grocery_and_Gourmet_Food_5.csv\n",
      "frame shape (133740, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (129605, 2)\n",
      "reviews_Health_and_Personal_Care_5.csv\n",
      "frame shape (313101, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (302347, 2)\n",
      "reviews_Home_and_Kitchen_5_0.csv\n",
      "frame shape (200000, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (192504, 2)\n",
      "reviews_Home_and_Kitchen_5_1.csv\n",
      "frame shape (200000, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (193216, 2)\n",
      "reviews_Musical_Instruments_5.csv\n",
      "frame shape (9489, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (9150, 2)\n",
      "reviews_Office_Products_5.csv\n",
      "frame shape (48198, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (47509, 2)\n",
      "reviews_Patio_Lawn_and_Garden_5.csv\n",
      "frame shape (11613, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (11468, 2)\n",
      "reviews_Pet_Supplies_5.csv\n",
      "frame shape (141903, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (136106, 2)\n",
      "reviews_Sports_and_Outdoors_5.csv\n",
      "frame shape (272266, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (261543, 2)\n",
      "reviews_Tools_and_Home_Improvement_5.csv\n",
      "frame shape (123707, 2) Index(['clean_text', 'sentiment'], dtype='object')\n",
      "frame shape (120052, 2)\n",
      "final shape (4156457, 2)\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "\n",
    "for f in cleaned_files:\n",
    "    print(f)\n",
    "    frame = pd.read_csv(\"data/\"+f, encoding='utf-8')\n",
    "    print(\"frame shape\", frame.shape, frame.columns)\n",
    "    \n",
    "    frame = frame[(frame.clean_text.notnull()) & (frame.clean_text.str.len() > 100)]\n",
    "    print(\"frame shape\", frame.shape)\n",
    "    frames.append(frame)\n",
    "    \n",
    "data = pd.concat(frames)\n",
    "print(\"final shape\", data.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_text    i highly recommend this series it is a must fo...\n",
      "sentiment                                                     1\n",
      "Name: 1, dtype: object\n",
      "clean_text    these speaker have a small footprint and big s...\n",
      "sentiment                                                     1\n",
      "Name: 125914, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import sklearn.utils\n",
    "print(data.iloc[1])\n",
    "\n",
    "data = sklearn.utils.shuffle(data, random_state=1)\n",
    "print(data.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 0\n",
    "split_size = 16384\n",
    "shuffled_frames = []\n",
    "while start_index+split_size < data.shape[0]:\n",
    "    shuffled_frames.append(data[start_index:start_index+split_size])\n",
    "    start_index += split_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch selected 40 (16384, 2)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 50 is out of bounds for axis 0 with size 50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-6877e85480f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffled_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch selected\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_to_bow_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0;31m# arg is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         return self._constructor(new_values,\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas/_libs/lib.c:66645)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8b62ab43a173>\u001b[0m in \u001b[0;36mreview_to_bow_vector\u001b[0;34m(rev)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mfirstFile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexCounter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordsList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mfirstFile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexCounter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m399999\u001b[0m \u001b[0;31m#Vector for unknown words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 50 is out of bounds for axis 0 with size 50"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(40, 50):\n",
    "    file_name = \"reviews_shuffled_\" + str(i)\n",
    "    label_file_name = \"labels_shuffled_\" + str(i)\n",
    "    batch = shuffled_frames[i]\n",
    "    print(\"batch selected\",i, batch.shape)\n",
    "    vecs = batch.clean_text.map(review_to_bow_vector)\n",
    "\n",
    "\n",
    "    arr = np.zeros((split_size, maxSeqLength))\n",
    "    labels = np.zeros((split_size, numClasses))\n",
    "\n",
    "    for j, v in enumerate(vecs):\n",
    "        arr[j,] = v\n",
    "        labels[j,] = one_hot_label(batch.sentiment.iloc[j])\n",
    "\n",
    "    np.save(\"data/shuffled/\" + file_name, arr)\n",
    "    np.save(\"data/shuffled/\" + label_file_name, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google news embedding vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"w2v/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s> 3000000\n",
      "in 2999999\n",
      "for 2999998\n",
      "that 2999997\n",
      "is 2999996\n",
      "on 2999995\n",
      "## 2999994\n",
      "The 2999993\n",
      "with 2999992\n",
      "said 2999991\n",
      "was 2999990\n",
      "the 2999989\n",
      "at 2999988\n",
      "not 2999987\n",
      "as 2999986\n",
      "it 2999985\n",
      "be 2999984\n",
      "from 2999983\n",
      "by 2999982\n",
      "are 2999981\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for word, vocab_obj in model.vocab.items():\n",
    "    \n",
    "    print(word, vocab_obj.count)\n",
    "    \n",
    "    if i % 20 == 19:\n",
    "        break\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_word(w):\n",
    "    \"\"\"\n",
    "        Only want lower case words that aren't stopwords and arent tags or other nonsense like ####\n",
    "    \"\"\"\n",
    "    if not w.lower() == w:\n",
    "        return False\n",
    "#     if w in stop_words:\n",
    "#         return False\n",
    "    if re.search(\"[^(\\w|\\'|\\-)]\", w):\n",
    "        return False\n",
    "#     if not w in model.wv.vocab:\n",
    "#         # shouldnt have to add this but for some reason it makes a difference, where are these words coming from?\n",
    "#         return False\n",
    "    \n",
    "    return True\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found counts\n",
      "vocab set\n"
     ]
    }
   ],
   "source": [
    "vocab_counts = [(word, vocab_obj.count) for  (word, vocab_obj) in model.vocab.items() if is_valid_word(word)]\n",
    "vocab_counts = sorted(vocab_counts, key=lambda x:x[1], reverse=True)\n",
    "print(\"found counts\")\n",
    "# needs a list for ordering\n",
    "final_vocab = [v[0] for v in vocab_counts[0:NUM_WORDS]]\n",
    "print(\"vocab set\")\n",
    "# get a lookup for O[1] access\n",
    "final_vocab_lookup = {v:final_vocab.index(v) for v in final_vocab}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_lookup = {v: k for k, v in final_vocab_lookup.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n",
      "for\n",
      "that\n",
      "is\n",
      "on\n",
      "with\n",
      "said\n",
      "was\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for v in final_vocab_lookup:\n",
    "    print(v)\n",
    "    count += 1\n",
    "    if count % 10 == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'for',\n",
       " 'that',\n",
       " 'is',\n",
       " 'on',\n",
       " 'with',\n",
       " 'said',\n",
       " 'was',\n",
       " 'the',\n",
       " 'at',\n",
       " 'not',\n",
       " 'as',\n",
       " 'it',\n",
       " 'be',\n",
       " 'from',\n",
       " 'by',\n",
       " 'are',\n",
       " 'have',\n",
       " 'he',\n",
       " 'will']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vocab[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.zeros((NUM_WORDS, 300))\n",
    "\n",
    "for i, v in enumerate(final_vocab):\n",
    "    if v not in model.wv.vocab:\n",
    "        print(\"huh\", v)\n",
    "    word_vectors[i,] = model.wv[v]\n",
    "    \n",
    "np.save(\"data/w2v_vectors.npy\", word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch selected 75 (16384, 2)\n",
      "saving arr (16384, 50)\n",
      "batch selected 76 (16384, 2)\n",
      "saving arr (28678, 50)\n",
      "batch selected 77 (16384, 2)\n",
      "saving arr (28734, 50)\n",
      "batch selected 78 (16384, 2)\n",
      "saving arr (28662, 50)\n",
      "batch selected 79 (16384, 2)\n",
      "saving arr (28764, 50)\n",
      "batch selected 80 (16384, 2)\n",
      "saving arr (28790, 50)\n",
      "batch selected 81 (16384, 2)\n",
      "saving arr (28592, 50)\n",
      "batch selected 82 (16384, 2)\n",
      "saving arr (28638, 50)\n",
      "batch selected 83 (16384, 2)\n",
      "saving arr (28690, 50)\n",
      "batch selected 84 (16384, 2)\n",
      "saving arr (28714, 50)\n",
      "batch selected 85 (16384, 2)\n",
      "saving arr (28728, 50)\n",
      "batch selected 86 (16384, 2)\n",
      "saving arr (28856, 50)\n",
      "batch selected 87 (16384, 2)\n",
      "saving arr (28688, 50)\n",
      "batch selected 88 (16384, 2)\n",
      "saving arr (28712, 50)\n",
      "batch selected 89 (16384, 2)\n",
      "saving arr (28776, 50)\n",
      "batch selected 90 (16384, 2)\n",
      "saving arr (28782, 50)\n",
      "batch selected 91 (16384, 2)\n",
      "saving arr (28758, 50)\n",
      "batch selected 92 (16384, 2)\n",
      "saving arr (28814, 50)\n",
      "batch selected 93 (16384, 2)\n",
      "saving arr (28726, 50)\n",
      "batch selected 94 (16384, 2)\n",
      "saving arr (28644, 50)\n",
      "batch selected 95 (16384, 2)\n",
      "saving arr (28716, 50)\n",
      "batch selected 96 (16384, 2)\n",
      "saving arr (28614, 50)\n",
      "batch selected 97 (16384, 2)\n",
      "saving arr (28728, 50)\n",
      "batch selected 98 (16384, 2)\n",
      "saving arr (28798, 50)\n",
      "batch selected 99 (16384, 2)\n",
      "saving arr (28668, 50)\n"
     ]
    }
   ],
   "source": [
    "review_sizes = collections.defaultdict(int)\n",
    "first = True\n",
    "def review_to_w2v_vector(rev):\n",
    "    split_words = rev.split()\n",
    "    \n",
    "    words_in_order = np.random.randint(len(final_vocab), size=maxSeqLength, dtype='int32')\n",
    "    indexCounter = 0\n",
    "    good_words = [w for w in split_words if w in final_vocab_lookup]\n",
    "    review_sizes[len(good_words)] += 1\n",
    "    \n",
    "    for word in good_words:\n",
    "        words_in_order[indexCounter] = final_vocab_lookup[word]\n",
    "        indexCounter += 1\n",
    "        if indexCounter >= maxSeqLength:\n",
    "            break\n",
    "    return words_in_order\n",
    "\n",
    "\n",
    "for i in range(75, 100):\n",
    "    \n",
    "    file_name = \"balanced_w2vreviews_\" + str(i)\n",
    "    label_file_name = \"balanced_w2vlabels_\" + str(i)\n",
    "    batch = shuffled_frames[i]\n",
    "    \n",
    "    \n",
    "    print(\"batch selected\",i, batch.shape)\n",
    "    vecs = batch.clean_text.map(review_to_w2v_vector)\n",
    "\n",
    "\n",
    "    arr = np.zeros((split_size, maxSeqLength))\n",
    "    labels = np.zeros((split_size, numClasses))\n",
    "\n",
    "    for j, v in enumerate(vecs):\n",
    "        arr[j,] = v\n",
    "        labels[j,] = one_hot_label(batch.sentiment.iloc[j])\n",
    "        \n",
    "        \n",
    "    if not first:\n",
    "        # dont oversample, test set\n",
    "        sm = SMOTE(ratio='minority', random_state=42)\n",
    "        arr, new_labels = sm.fit_sample(arr, batch.sentiment.values)\n",
    "        labels = np.zeros((arr.shape[0], numClasses))\n",
    "        for i, l in enumerate(new_labels):\n",
    "            labels[i,] = one_hot_label(new_labels[i])\n",
    "    else:\n",
    "        first = False\n",
    "        file_name = file_name.replace(\"balanced\", \"test\")\n",
    "        label_file_name = label_file_name.replace(\"balanced\", \"test\")\n",
    "    \n",
    "    print(\"saving arr\", arr.shape)\n",
    "    np.save(\"data/vecs/\" + file_name, arr)\n",
    "    np.save(\"data/vecs/\" + label_file_name, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(review_sizes.items())\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this product suck it will look bad in a matter of day with normal usage you will see small scratch which will drive you crazy get the lifetime protector from best buy 25 dollar\n",
      "this\n",
      "product\n",
      "suck\n",
      "it\n",
      "will\n",
      "look\n",
      "bad\n",
      "in\n",
      "matter\n",
      "day\n",
      "with\n",
      "normal\n",
      "usage\n",
      "you\n",
      "will\n",
      "see\n",
      "small\n",
      "scratch\n",
      "which\n",
      "will\n",
      "drive\n",
      "you\n",
      "crazy\n",
      "get\n",
      "the\n",
      "lifetime\n",
      "protector\n",
      "from\n",
      "best\n",
      "buy\n",
      "dollar\n",
      "pawn\n",
      "cobblestone_alleyways\n",
      "decree_appointing\n",
      "shrimpers\n",
      "city_slickers\n",
      "tentatively_slated\n",
      "sociétés\n",
      "revolver_sawed_off\n",
      "groupage\n",
      "perkier\n",
      "3years\n",
      "banners\n",
      "toothpaste_toothbrushes\n",
      "vomit_inducing\n",
      "toilets_sinks\n",
      "signaling_pathways\n",
      "walkers_joggers\n",
      "hotch_potch\n",
      "swordfishing\n"
     ]
    }
   ],
   "source": [
    "_ = np.random.randint(batch.shape[0])\n",
    "print(batch.iloc[_][\"clean_text\"])\n",
    "for w in arr[_,]:\n",
    "    print(reverse_lookup[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28560, 50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16384,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = labels[:,0]\n",
    "sentiment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [28560, 16384]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f727a50dce68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0msentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2029\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [28560, 16384]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(arr,  sentiment, test_size=1.0/6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_data, train_labels)\n",
    "preds = lr.predict(test_data)\n",
    "print(\"accuracy\", np.mean(preds==test_labels))\n",
    "print(\"balance\", np.mean(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_data, train_labels)\n",
    "rf_preds = rf.predict(test_data)\n",
    "print(\"rf accuracy\", np.mean(preds==test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
