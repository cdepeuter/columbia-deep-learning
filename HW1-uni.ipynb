{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# COMS 4995_002 Deep Learning Assignment 1\n",
    "Due on Monday, Oct 9, 11:59pm\n",
    "\n",
    "This assignment can be done in groups of at most 3 students. Everyone must submit on Courseworks individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the UNIs of your group (if applicable)\n",
    "\n",
    "Member 1: Name, UNI\n",
    "\n",
    "Member 2: Name, UNI\n",
    "\n",
    "Member 3: Name, UNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        eps = .01\n",
    "        self.parameters = {'layer_dimensions':layer_dimensions, 'drop_prob':drop_prob, 'reg_lambda':reg_lambda} \n",
    "        self.num_layers = len(layer_dimensions)\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        \n",
    "        # init parameters\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.parameters['W' + str(l)] = np.random.randn(layer_dimensions[l], layer_dimensions[l-1]) * eps\n",
    "            self.parameters['b' + str(l)]= np.zeros((layer_dimensions[l], 1))\n",
    "    \n",
    "        # print all params:\n",
    "        print(self.parameters.keys())\n",
    "#     def gradientCheck(self, theta, x):\n",
    "#         eps = 1e-7\n",
    "#         J_pos = J(theta + eps, x)\n",
    "#         J_neg = J(theta - eps, x)\n",
    "#         numerical_deriv = (J_pos - J-neg)/(2*eps)\n",
    "#         analytic_deriv = JDeriv(theta, x)\n",
    "#         error = norm(numerical_deriv-analyic_deriv)/norm()\n",
    "#         return error\n",
    "        \n",
    "    def affineForward(self, A_, W_, b_):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "        print(\"forward, shape W\", W_.shape)\n",
    "        print(\"forward, shape A\", A_.shape)\n",
    "        print(\"forward, shape b\", b_.shape)\n",
    "        return np.dot(W_, A_) + b_, [A_, W_, b_]\n",
    "        \n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\" \n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            return self.relu(A)\n",
    "        if activation == \"softmax\":\n",
    "            return self.softmax(A)\n",
    "        \n",
    "        return A\n",
    "\n",
    "    def softmax(self, X):\n",
    "        # softmax\n",
    "        return np.exp(X)/np.sum(np.exp(X), axis=0)\n",
    "    \n",
    "    def relu(self, X):\n",
    "        return np.maximum(0,X)\n",
    "            \n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: \n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        \n",
    "        # make a mask, binary mask\n",
    "        # where that mask is 0, drop those activation units\n",
    "        # where its one, scale them up by 1/(1-p)\n",
    "        # in forward we need to apply this mask\n",
    "\n",
    "        return A, M\n",
    "\n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        cache = {}\n",
    "        A = X\n",
    "        \n",
    "        for l in range(1, self.num_layers):\n",
    "            print(\"forward, layer\", l)\n",
    "            print(\"A SHAPE\", A.shape)\n",
    "            layer_cache = {}\n",
    "            Z, cache_l = self.affineForward(A, self.parameters[\"W\" + str(l)], self.parameters[\"b\" + str(l)])\n",
    "            \n",
    "            layer_cache[\"linear\"] = cache_l\n",
    "            if l != self.num_layers - 1:\n",
    "                cache_a = self.activationForward(Z)\n",
    "            else:\n",
    "                cache_a = self.activationForward(Z, activation=\"softmax\")\n",
    "                \n",
    "            layer_cache['activation'] = cache_a\n",
    "            cache[str(l)] = layer_cache\n",
    "            A = cache_a\n",
    "        \n",
    "        return A, cache\n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # compute loss\n",
    "        #+ np.multiply((1-Y), np.log(1-preds))\n",
    "        # one hot encode true labels\n",
    "        # only look at how close we are to previous labels\n",
    "        true_labels = one_hot(y)\n",
    "        preds = np.multiply(AL, true_labels)\n",
    "        print(\"na in AL?\", np.sum(np.isnan(AL)), \"max in AL\", np.max(AL))\n",
    "        print(\"na in preds?\", np.sum(np.isnan(preds)))\n",
    "        \n",
    "        cost = -np.sum(np.multiply(true_labels, np.log(AL)) + np.multiply((1-true_labels), np.log(1-AL)))/ true_labels.shape[1]\n",
    "\n",
    "        if self.reg_lambda > 0:\n",
    "            # add regularization\n",
    "            pass\n",
    "        \n",
    "        # gradient of cost\n",
    "        dAL = np.multiply(true_labels - preds, true_labels)\n",
    "        \n",
    "        return cost, dAL\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        A, W, b = cache['linear']\n",
    "        \n",
    "\n",
    "        # activation backwards\n",
    "        dZ_l = self.activationBackward(dA_prev, cache)\n",
    "        \n",
    "        dA = np.dot(W.T, dZ_l)\n",
    "        dW = (1/dA_prev.shape[0]) * np.dot(dZ_l, A.T)\n",
    "        db = (1/dA_prev.shape[0]) * np.sum(dZ_l, axis=1, keepdims=True)\n",
    "        print(\"affine backward, db shape\", db.shape)\n",
    "        return dA, dW, db\n",
    "\n",
    "    def activationBackward(self, dA, cache, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        \n",
    "        #dzl = dAl*gl_p(zl)\n",
    "        # only passed in the cached linear layer\n",
    "        \n",
    "        # zl is avation at layer l, inputted as cache\n",
    "       \n",
    "        A, W, b = cache['linear']\n",
    "        print(\"backwards, W shape\", W.shape)\n",
    "        print(\"backwards, A shape\", A.shape)\n",
    "        print(\"backwards, b shape\", W.shape)\n",
    "        zl =  np.dot(W, A) + b\n",
    "        # dA is derivative last layer, first time it will be cost\n",
    "        return np.multiply(dA , self.relu_derivative(zl))\n",
    "        \n",
    "    def relu_derivative(self, dx):\n",
    "        return 1.0 * (dx > 0)\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "\n",
    "        # in backwards we need to apply that mask to the derivatives\n",
    "        # so cache here contains the dropout mask you used in forward prop\n",
    "        return dA\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        dA_prev = dAL\n",
    "        for l in range(self.num_layers-1, 0, -1):\n",
    "            cached_l = cache[str(l)]\n",
    "            # [A, W, b] for layer\n",
    "            print(\"backprop, layer:\", l)\n",
    "            \n",
    "            # affine backwards\n",
    "            dA, dW, db = self.affineBackward(dA_prev ,cached_l)\n",
    "            gradients[\"dW\" + str(l)] = dW\n",
    "            gradients[\"db\" + str(l)] = db\n",
    "        \n",
    "           \n",
    "            if self.drop_prob > 0:\n",
    "                #call dropout_backward\n",
    "                dA_prev = self.dropout_backward(dAL, cache)\n",
    "            else:\n",
    "                dA_prev = dA\n",
    "            \n",
    "            \n",
    "        if self.reg_lambda > 0:\n",
    "            # add gradients from L2 regularization to each dW\n",
    "            pass\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "        L = self.num_layers\n",
    "        print(\"updating params\")\n",
    "        # update by adding alpha * gradient to the params\n",
    "        for l in range(1,L):\n",
    "            print(\"W\" + str(l) +\" old shape\", self.parameters[\"W\" + str(l)].shape)\n",
    "            self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - alpha * gradients[\"dW\" + str(l)]\n",
    "            print(\"W\" + str(l) +\" new shape\", self.parameters[\"W\" + str(l)].shape)\n",
    "\n",
    "            print(\"b\" + str(l) +\" old shape\", self.parameters[\"b\" + str(l)].shape)\n",
    "            self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - alpha * gradients[\"db\" + str(l)]\n",
    "            print(\"b\" + str(l) +\" new shape\", self.parameters[\"b\" + str(l)].shape)\n",
    "            \n",
    "    def train(self, X, y, iters=1000, alpha=0.0001, batch_size=100, print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "        # we should get validation\n",
    "        \n",
    "        print(\"X train shape:\", X_train.shape)\n",
    "        print(\"y train shape:\", y_train.shape)\n",
    "        # get minibatch\n",
    "#         batch_x, batch_y = self.get_batch(X, y, batch_size)\n",
    "#         print(\"batch 1 x, shape:\", batch_x.shape)\n",
    "        \n",
    "        for i in range(0, iters):\n",
    "            # should we change alpha each step?\n",
    "            \n",
    "            \n",
    "            # forward prop\n",
    "            last_layer, cache = self.forwardPropagation(X)\n",
    "\n",
    "            # compute loss\n",
    "            cost, cost_deriv = self.costFunction(last_layer, y)\n",
    "\n",
    "            # compute gradients\n",
    "            gradients = self.backPropagation(cost_deriv, y, cache)\n",
    "            \n",
    "            # update weights and biases based on gradient\n",
    "            self.updateParameters(gradients, alpha)\n",
    "            \n",
    "            if i % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                print(\"COST\", cost)\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = self.forwardPropagation(x)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "        # are minibatches random\n",
    "        start_batch = np.random.randint(0, X.shape[0]-batch_size)\n",
    "        print(\"batch start:\" , start_batch, start_batch + batch_size)\n",
    "        X_batch = X[:,start_batch]\n",
    "        y_batch = y[:,start_batch]\n",
    "        print(\"X batch shape:\", X_batch.shape)\n",
    "        print(\"y batch shape:\", y_batch.shape)\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions, DO NOT modify this\n",
    "\n",
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to load data, DO NOT change these\n",
    "\n",
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((num_classes, y.shape[0]))\n",
    "    y_one_hot[y, range(y.shape[0])] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip() for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "\n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw1/'\n",
    "#X_train, y_train = get_train_data(data_root_path) # this may take a few minutes\n",
    "#X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = y_train[0:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "#### Simple fully-connected deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['b2', 'W3', 'W2', 'reg_lambda', 'W1', 'layer_dimensions', 'drop_prob', 'b3', 'b1'])\n",
      "X train shape: (3072, 50000)\n",
      "y train shape: (50000,)\n",
      "forward, layer 1\n",
      "A SHAPE (3072, 50000)\n",
      "forward, shape W (8, 3072)\n",
      "forward, shape A (3072, 50000)\n",
      "forward, shape b (8, 1)\n",
      "forward, layer 2\n",
      "A SHAPE (8, 50000)\n",
      "forward, shape W (5, 8)\n",
      "forward, shape A (8, 50000)\n",
      "forward, shape b (5, 1)\n",
      "forward, layer 3\n",
      "A SHAPE (5, 50000)\n",
      "forward, shape W (10, 5)\n",
      "forward, shape A (5, 50000)\n",
      "forward, shape b (10, 1)\n",
      "na in AL? 0 max in AL 0.100043173757\n",
      "na in preds? 0\n",
      "backprop, layer: 3\n",
      "backwards, W shape (10, 5)\n",
      "backwards, A shape (5, 50000)\n",
      "backwards, b shape (10, 5)\n",
      "affine backward, db shape (10, 1)\n",
      "backprop, layer: 2\n",
      "backwards, W shape (5, 8)\n",
      "backwards, A shape (8, 50000)\n",
      "backwards, b shape (5, 8)\n",
      "affine backward, db shape (5, 1)\n",
      "backprop, layer: 1\n",
      "backwards, W shape (8, 3072)\n",
      "backwards, A shape (3072, 50000)\n",
      "backwards, b shape (8, 3072)\n",
      "affine backward, db shape (8, 1)\n",
      "updating params\n",
      "W1 old shape (8, 3072)\n",
      "W1 new shape (8, 3072)\n",
      "b1 old shape (8, 1)\n",
      "b1 new shape (8, 1)\n",
      "W2 old shape (5, 8)\n",
      "W2 new shape (5, 8)\n",
      "b2 old shape (5, 1)\n",
      "b2 new shape (5, 1)\n",
      "W3 old shape (10, 5)\n",
      "W3 new shape (10, 5)\n",
      "b3 old shape (10, 1)\n",
      "b3 new shape (10, 1)\n",
      "COST 3.25083096227\n",
      "forward, layer 1\n",
      "A SHAPE (3072, 50000)\n",
      "forward, shape W (8, 3072)\n",
      "forward, shape A (3072, 50000)\n",
      "forward, shape b (8, 1)\n",
      "forward, layer 2\n",
      "A SHAPE (8, 50000)\n",
      "forward, shape W (5, 8)\n",
      "forward, shape A (8, 50000)\n",
      "forward, shape b (5, 1)\n",
      "forward, layer 3\n",
      "A SHAPE (5, 50000)\n",
      "forward, shape W (10, 5)\n",
      "forward, shape A (5, 50000)\n",
      "forward, shape b (10, 1)\n",
      "na in AL? 0 max in AL 0.169613863788\n",
      "na in preds? 0\n",
      "backprop, layer: 3\n",
      "backwards, W shape (10, 5)\n",
      "backwards, A shape (5, 50000)\n",
      "backwards, b shape (10, 5)\n",
      "affine backward, db shape (10, 1)\n",
      "backprop, layer: 2\n",
      "backwards, W shape (5, 8)\n",
      "backwards, A shape (8, 50000)\n",
      "backwards, b shape (5, 8)\n",
      "affine backward, db shape (5, 1)\n",
      "backprop, layer: 1\n",
      "backwards, W shape (8, 3072)\n",
      "backwards, A shape (3072, 50000)\n",
      "backwards, b shape (8, 3072)\n",
      "affine backward, db shape (8, 1)\n",
      "updating params\n",
      "W1 old shape (8, 3072)\n",
      "W1 new shape (8, 3072)\n",
      "b1 old shape (8, 1)\n",
      "b1 new shape (8, 1)\n",
      "W2 old shape (5, 8)\n",
      "W2 new shape (5, 8)\n",
      "b2 old shape (5, 1)\n",
      "b2 new shape (5, 1)\n",
      "W3 old shape (10, 5)\n",
      "W3 new shape (10, 5)\n",
      "b3 old shape (10, 1)\n",
      "b3 new shape (10, 1)\n",
      "forward, layer 1\n",
      "A SHAPE (3072, 50000)\n",
      "forward, shape W (8, 3072)\n",
      "forward, shape A (3072, 50000)\n",
      "forward, shape b (8, 1)\n",
      "forward, layer 2\n",
      "A SHAPE (8, 50000)\n",
      "forward, shape W (5, 8)\n",
      "forward, shape A (8, 50000)\n",
      "forward, shape b (5, 1)\n",
      "forward, layer 3\n",
      "A SHAPE (5, 50000)\n",
      "forward, shape W (10, 5)\n",
      "forward, shape A (5, 50000)\n",
      "forward, shape b (10, 1)\n",
      "na in AL? 0 max in AL 0.169613863788\n",
      "na in preds? 0\n",
      "backprop, layer: 3\n",
      "backwards, W shape (10, 5)\n",
      "backwards, A shape (5, 50000)\n",
      "backwards, b shape (10, 5)\n",
      "affine backward, db shape (10, 1)\n",
      "backprop, layer: 2\n",
      "backwards, W shape (5, 8)\n",
      "backwards, A shape (8, 50000)\n",
      "backwards, b shape (5, 8)\n",
      "affine backward, db shape (5, 1)\n",
      "backprop, layer: 1\n",
      "backwards, W shape (8, 3072)\n",
      "backwards, A shape (3072, 50000)\n",
      "backwards, b shape (8, 3072)\n",
      "affine backward, db shape (8, 1)\n",
      "updating params\n",
      "W1 old shape (8, 3072)\n",
      "W1 new shape (8, 3072)\n",
      "b1 old shape (8, 1)\n",
      "b1 new shape (8, 1)\n",
      "W2 old shape (5, 8)\n",
      "W2 new shape (5, 8)\n",
      "b2 old shape (5, 1)\n",
      "b2 new shape (5, 1)\n",
      "W3 old shape (10, 5)\n",
      "W3 new shape (10, 5)\n",
      "b3 old shape (10, 1)\n",
      "b3 new shape (10, 1)\n",
      "forward, layer 1\n",
      "A SHAPE (3072, 50000)\n",
      "forward, shape W (8, 3072)\n",
      "forward, shape A (3072, 50000)\n",
      "forward, shape b (8, 1)\n",
      "forward, layer 2\n",
      "A SHAPE (8, 50000)\n",
      "forward, shape W (5, 8)\n",
      "forward, shape A (8, 50000)\n",
      "forward, shape b (5, 1)\n",
      "forward, layer 3\n",
      "A SHAPE (5, 50000)\n",
      "forward, shape W (10, 5)\n",
      "forward, shape A (5, 50000)\n",
      "forward, shape b (10, 1)\n",
      "na in AL? 0 max in AL 0.169613863788\n",
      "na in preds? 0\n",
      "backprop, layer: 3\n",
      "backwards, W shape (10, 5)\n",
      "backwards, A shape (5, 50000)\n",
      "backwards, b shape (10, 5)\n",
      "affine backward, db shape (10, 1)\n",
      "backprop, layer: 2\n",
      "backwards, W shape (5, 8)\n",
      "backwards, A shape (8, 50000)\n",
      "backwards, b shape (5, 8)\n",
      "affine backward, db shape (5, 1)\n",
      "backprop, layer: 1\n",
      "backwards, W shape (8, 3072)\n",
      "backwards, A shape (3072, 50000)\n",
      "backwards, b shape (8, 3072)\n",
      "affine backward, db shape (8, 1)\n",
      "updating params\n",
      "W1 old shape (8, 3072)\n",
      "W1 new shape (8, 3072)\n",
      "b1 old shape (8, 1)\n",
      "b1 new shape (8, 1)\n",
      "W2 old shape (5, 8)\n",
      "W2 new shape (5, 8)\n",
      "b2 old shape (5, 1)\n",
      "b2 new shape (5, 1)\n",
      "W3 old shape (10, 5)\n",
      "W3 new shape (10, 5)\n",
      "b3 old shape (10, 1)\n",
      "b3 new shape (10, 1)\n",
      "forward, layer 1\n",
      "A SHAPE (3072, 50000)\n",
      "forward, shape W (8, 3072)\n",
      "forward, shape A (3072, 50000)\n",
      "forward, shape b (8, 1)\n",
      "forward, layer 2\n",
      "A SHAPE (8, 50000)\n",
      "forward, shape W (5, 8)\n",
      "forward, shape A (8, 50000)\n",
      "forward, shape b (5, 1)\n",
      "forward, layer 3\n",
      "A SHAPE (5, 50000)\n",
      "forward, shape W (10, 5)\n",
      "forward, shape A (5, 50000)\n",
      "forward, shape b (10, 1)\n",
      "na in AL? 0 max in AL 0.169613863788\n",
      "na in preds? 0\n",
      "backprop, layer: 3\n",
      "backwards, W shape (10, 5)\n",
      "backwards, A shape (5, 50000)\n",
      "backwards, b shape (10, 5)\n",
      "affine backward, db shape (10, 1)\n",
      "backprop, layer: 2\n",
      "backwards, W shape (5, 8)\n",
      "backwards, A shape (8, 50000)\n",
      "backwards, b shape (5, 8)\n",
      "affine backward, db shape (5, 1)\n",
      "backprop, layer: 1\n",
      "backwards, W shape (8, 3072)\n",
      "backwards, A shape (3072, 50000)\n",
      "backwards, b shape (8, 3072)\n",
      "affine backward, db shape (8, 1)\n",
      "updating params\n",
      "W1 old shape (8, 3072)\n",
      "W1 new shape (8, 3072)\n",
      "b1 old shape (8, 1)\n",
      "b1 new shape (8, 1)\n",
      "W2 old shape (5, 8)\n",
      "W2 new shape (5, 8)\n",
      "b2 old shape (5, 1)\n",
      "b2 new shape (5, 1)\n",
      "W3 old shape (10, 5)\n",
      "W3 new shape (10, 5)\n",
      "b3 old shape (10, 1)\n",
      "b3 new shape (10, 1)\n",
      "forward, layer 1\n",
      "A SHAPE (3072, 50000)\n",
      "forward, shape W (8, 3072)\n",
      "forward, shape A (3072, 50000)\n",
      "forward, shape b (8, 1)\n",
      "forward, layer 2\n",
      "A SHAPE (8, 50000)\n",
      "forward, shape W (5, 8)\n",
      "forward, shape A (8, 50000)\n",
      "forward, shape b (5, 1)\n",
      "forward, layer 3\n",
      "A SHAPE (5, 50000)\n",
      "forward, shape W (10, 5)\n",
      "forward, shape A (5, 50000)\n",
      "forward, shape b (10, 1)\n",
      "na in AL? 0 max in AL 0.169613863788\n",
      "na in preds? 0\n",
      "backprop, layer: 3\n",
      "backwards, W shape (10, 5)\n",
      "backwards, A shape (5, 50000)\n",
      "backwards, b shape (10, 5)\n",
      "affine backward, db shape (10, 1)\n",
      "backprop, layer: 2\n",
      "backwards, W shape (5, 8)\n",
      "backwards, A shape (8, 50000)\n",
      "backwards, b shape (5, 8)\n",
      "affine backward, db shape (5, 1)\n",
      "backprop, layer: 1\n",
      "backwards, W shape (8, 3072)\n",
      "backwards, A shape (3072, 50000)\n",
      "backwards, b shape (8, 3072)\n",
      "affine backward, db shape (8, 1)\n",
      "updating params\n",
      "W1 old shape (8, 3072)\n",
      "W1 new shape (8, 3072)\n",
      "b1 old shape (8, 1)\n",
      "b1 new shape (8, 1)\n",
      "W2 old shape (5, 8)\n",
      "W2 new shape (5, 8)\n",
      "b2 old shape (5, 1)\n",
      "b2 new shape (5, 1)\n",
      "W3 old shape (10, 5)\n",
      "W3 new shape (10, 5)\n",
      "b3 old shape (10, 1)\n",
      "b3 new shape (10, 1)\n",
      "forward, layer 1\n",
      "A SHAPE (3072, 50000)\n",
      "forward, shape W (8, 3072)\n",
      "forward, shape A (3072, 50000)\n",
      "forward, shape b (8, 1)\n",
      "forward, layer 2\n",
      "A SHAPE (8, 50000)\n",
      "forward, shape W (5, 8)\n",
      "forward, shape A (8, 50000)\n",
      "forward, shape b (5, 1)\n",
      "forward, layer 3\n",
      "A SHAPE (5, 50000)\n",
      "forward, shape W (10, 5)\n",
      "forward, shape A (5, 50000)\n",
      "forward, shape b (10, 1)\n",
      "na in AL? 0 max in AL 0.169613863788\n",
      "na in preds? 0\n",
      "backprop, layer: 3\n",
      "backwards, W shape (10, 5)\n",
      "backwards, A shape (5, 50000)\n",
      "backwards, b shape (10, 5)\n",
      "affine backward, db shape (10, 1)\n",
      "backprop, layer: 2\n",
      "backwards, W shape (5, 8)\n",
      "backwards, A shape (8, 50000)\n",
      "backwards, b shape (5, 8)\n",
      "affine backward, db shape (5, 1)\n",
      "backprop, layer: 1\n",
      "backwards, W shape (8, 3072)\n",
      "backwards, A shape (3072, 50000)\n",
      "backwards, b shape (8, 3072)\n",
      "affine backward, db shape (8, 1)\n",
      "updating params\n",
      "W1 old shape (8, 3072)\n",
      "W1 new shape (8, 3072)\n",
      "b1 old shape (8, 1)\n",
      "b1 new shape (8, 1)\n",
      "W2 old shape (5, 8)\n",
      "W2 new shape (5, 8)\n",
      "b2 old shape (5, 1)\n",
      "b2 new shape (5, 1)\n",
      "W3 old shape (10, 5)\n",
      "W3 new shape (10, 5)\n",
      "b3 old shape (10, 1)\n",
      "b3 new shape (10, 1)\n",
      "forward, layer 1\n",
      "A SHAPE (3072, 50000)\n",
      "forward, shape W (8, 3072)\n",
      "forward, shape A (3072, 50000)\n",
      "forward, shape b (8, 1)\n",
      "forward, layer 2\n",
      "A SHAPE (8, 50000)\n",
      "forward, shape W (5, 8)\n",
      "forward, shape A (8, 50000)\n",
      "forward, shape b (5, 1)\n",
      "forward, layer 3\n",
      "A SHAPE (5, 50000)\n",
      "forward, shape W (10, 5)\n",
      "forward, shape A (5, 50000)\n",
      "forward, shape b (10, 1)\n",
      "na in AL? 0 max in AL 0.169613863788\n",
      "na in preds? 0\n",
      "backprop, layer: 3\n",
      "backwards, W shape (10, 5)\n",
      "backwards, A shape (5, 50000)\n",
      "backwards, b shape (10, 5)\n",
      "affine backward, db shape (10, 1)\n",
      "backprop, layer: 2\n",
      "backwards, W shape (5, 8)\n",
      "backwards, A shape (8, 50000)\n",
      "backwards, b shape (5, 8)\n",
      "affine backward, db shape (5, 1)\n",
      "backprop, layer: 1\n",
      "backwards, W shape (8, 3072)\n",
      "backwards, A shape (3072, 50000)\n",
      "backwards, b shape (8, 3072)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affine backward, db shape (8, 1)\n",
      "updating params\n",
      "W1 old shape (8, 3072)\n",
      "W1 new shape (8, 3072)\n",
      "b1 old shape (8, 1)\n",
      "b1 new shape (8, 1)\n",
      "W2 old shape (5, 8)\n",
      "W2 new shape (5, 8)\n",
      "b2 old shape (5, 1)\n",
      "b2 new shape (5, 1)\n",
      "W3 old shape (10, 5)\n",
      "W3 new shape (10, 5)\n",
      "b3 old shape (10, 1)\n",
      "b3 new shape (10, 1)\n",
      "forward, layer 1\n",
      "A SHAPE (3072, 50000)\n",
      "forward, shape W (8, 3072)\n",
      "forward, shape A (3072, 50000)\n",
      "forward, shape b (8, 1)\n",
      "forward, layer 2\n",
      "A SHAPE (8, 50000)\n",
      "forward, shape W (5, 8)\n",
      "forward, shape A (8, 50000)\n",
      "forward, shape b (5, 1)\n",
      "forward, layer 3\n",
      "A SHAPE (5, 50000)\n",
      "forward, shape W (10, 5)\n",
      "forward, shape A (5, 50000)\n",
      "forward, shape b (10, 1)\n",
      "na in AL? 0 max in AL 0.169613863788\n",
      "na in preds? 0\n",
      "backprop, layer: 3\n",
      "backwards, W shape (10, 5)\n",
      "backwards, A shape (5, 50000)\n",
      "backwards, b shape (10, 5)\n",
      "affine backward, db shape (10, 1)\n",
      "backprop, layer: 2\n",
      "backwards, W shape (5, 8)\n",
      "backwards, A shape (8, 50000)\n",
      "backwards, b shape (5, 8)\n",
      "affine backward, db shape (5, 1)\n",
      "backprop, layer: 1\n",
      "backwards, W shape (8, 3072)\n",
      "backwards, A shape (3072, 50000)\n",
      "backwards, b shape (8, 3072)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-b10ab949d04f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlayer_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# including the input and output layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_dimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-130-1a40ef89bf51>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, iters, alpha, batch_size, print_every)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;31m# compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_deriv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;31m# update weights and biases based on gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-1a40ef89bf51>\u001b[0m in \u001b[0;36mbackPropagation\u001b[0;34m(self, dAL, Y, cache)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# affine backwards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffineBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA_prev\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcached_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-1a40ef89bf51>\u001b[0m in \u001b[0;36maffineBackward\u001b[0;34m(self, dA_prev, cache)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mdZ_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivationBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mdA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "layer_dimensions = [X_train.shape[0],8,5, 10]  # including the input and output layers\n",
    "NN = NeuralNetwork(layer_dimensions)\n",
    "NN.train(X_train, y_train, iters=100, alpha=.01, batch_size=100, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted = NN.predict(X_test)\n",
    "save_predictions('ans1-uni', y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test if your numpy file has been saved correctly\n",
    "loaded_y = np.load('ans1-uni.npy')\n",
    "print(loaded_y.shape)\n",
    "loaded_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Regularizing the neural network\n",
    "#### Add dropout and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN2 = NeuralNetwork(layer_dimensions, drop_prob=0, reg_lambda=0)\n",
    "NN2.train(X_train, y_train, iters=100, alpha=0.00001, batch_size=1000, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted2 = NN2.predict(X)\n",
    "save_predictions(y_predicted, 'ans2-uni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, 0, 2, 5])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.random.randint(10, size=(6))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = one_hot(test)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
