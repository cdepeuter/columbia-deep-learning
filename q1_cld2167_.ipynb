{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "UNI = \"cld2167\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_array(path):\n",
    "    \"\"\"\n",
    "    Given path of image, returns it's numpy array\n",
    "    \"\"\"\n",
    "    return scipy.misc.imread(path)\n",
    "\n",
    "def get_files(folder):\n",
    "    \"\"\"\n",
    "    Given path to folder, returns list of files in it\n",
    "    \"\"\"\n",
    "    filenames = [file for file in glob.glob(folder+'*/*')]\n",
    "    filenames.sort()\n",
    "    return filenames\n",
    "\n",
    "def get_label(filepath, label2id):\n",
    "    \"\"\"\n",
    "    Files are assumed to be labeled as: /path/to/file/999_frog.png\n",
    "    Returns label for a filepath\n",
    "    \"\"\"\n",
    "    tokens = filepath.split('/')\n",
    "    label = tokens[-1].split('_')[1][:-4]\n",
    "    if label in label2id:\n",
    "        return label2id[label]\n",
    "    else:\n",
    "        sys.exit(\"Invalid label: \" + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(folder, label2id):\n",
    "    \"\"\"\n",
    "    Returns vector of labels extracted from filenames of all files in folder\n",
    "    :param folder: path to data folder\n",
    "    :param label2id: mapping of text labels to numeric ids. (Eg: automobile -> 0)\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    y = []\n",
    "    for f in files:\n",
    "        y.append(get_label(f,label2id))\n",
    "    return np.array(y)\n",
    "\n",
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((num_classes, y.shape[0]))\n",
    "    y_one_hot[y, range(y.shape[0])] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "def get_label_mapping(label_file):\n",
    "    \"\"\"\n",
    "    Returns mappings of label to index and index to label\n",
    "    The input file has list of labels, each on a separate line.\n",
    "    \"\"\"\n",
    "    with open(label_file, 'r') as f:\n",
    "        id2label = f.readlines()\n",
    "        id2label = [l.strip().split(\"-\")[1] for l in id2label]\n",
    "    label2id = {}\n",
    "    count = 0\n",
    "    for label in id2label:\n",
    "        label2id[label] = count\n",
    "        count += 1\n",
    "    return id2label, label2id\n",
    "\n",
    "def get_images(folder):\n",
    "    \"\"\"\n",
    "    returns numpy array of all samples in folder\n",
    "    each column is a sample resized to 30x30 and flattened\n",
    "    \"\"\"\n",
    "    files = get_files(folder)\n",
    "    images = []\n",
    "    count = 0\n",
    "\n",
    "    for f in files:\n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"Loaded {}/{}\".format(count,len(files)))\n",
    "        img_arr = get_img_array(f)\n",
    "        img_arr = img_arr.flatten() / 255.0\n",
    "        images.append(img_arr)\n",
    "    X = np.column_stack(images)\n",
    "\n",
    "    return X\n",
    "\n",
    "def get_train_data(data_root_path):\n",
    "    \"\"\"\n",
    "    Return X and y\n",
    "    \"\"\"\n",
    "    train_data_path = data_root_path + 'train'\n",
    "    id2label, label2id = get_label_mapping(data_root_path+'labels.txt')\n",
    "    print(label2id)\n",
    "    X = get_images(train_data_path)\n",
    "    y = get_labels(train_data_path, label2id)\n",
    "    return X, y\n",
    "\n",
    "def save_predictions(filename, y):\n",
    "    \"\"\"\n",
    "    Dumps y into .npy file\n",
    "    \"\"\"\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      "Loaded 10000/50000\n",
      "Loaded 20000/50000\n",
      "Loaded 30000/50000\n",
      "Loaded 40000/50000\n",
      "Loaded 50000/50000\n",
      "Loaded 10000/10000\n",
      "Data loading done\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_root_path = 'cifar10-hw1/'\n",
    "X_train_total, y_train_total = get_train_data(data_root_path) # this may take a few minutes\n",
    "X_test = get_images(data_root_path + 'test')\n",
    "print('Data loading done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=X_train_total[:,:45000]\n",
    "# X_val=X_train_total[:,45000:]\n",
    "# y_train=y_train_total[:45000]\n",
    "# y_val=y_train_total[45000:]\n",
    "\n",
    "## probably best to train on EVERYTHING\n",
    "\n",
    "\n",
    "X_train=X_train_total[:,:45000]\n",
    "X_val=X_train_total[:,45000:]\n",
    "y_train=y_train_total[:45000]\n",
    "y_val=y_train_total[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000,)\n",
      "(10, 45000)\n"
     ]
    }
   ],
   "source": [
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((num_classes, y.shape[0]))\n",
    "    y_one_hot[y, range(y.shape[0])] = 1\n",
    "    return y_one_hot\n",
    "print(y_train.shape)\n",
    "y_train=one_hot(y_train)\n",
    "y_val=one_hot(y_val)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X_train,y_train,batch_size=100):\n",
    "    ix = np.random.choice(X_train.shape[1], batch_size, replace=True)\n",
    "    X_train_batch=X_train[:,ix]\n",
    "    y_train_batch=y_train[:,ix]\n",
    "    return X_train_batch.T,y_train_batch.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 45000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, name):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.140000, loss 15.641579\n",
      "test accuracy 0.1044\n",
      "step 100, training accuracy 0.130000, loss 6.907915\n",
      "test accuracy 0.0962\n",
      "step 200, training accuracy 0.150000, loss 3.996650\n",
      "test accuracy 0.1072\n",
      "step 300, training accuracy 0.250000, loss 2.733493\n",
      "test accuracy 0.2298\n",
      "step 400, training accuracy 0.250000, loss 2.205670\n",
      "test accuracy 0.2734\n",
      "step 500, training accuracy 0.250000, loss 2.077400\n",
      "test accuracy 0.2774\n",
      "step 600, training accuracy 0.200000, loss 2.172625\n",
      "test accuracy 0.2818\n",
      "step 700, training accuracy 0.330000, loss 1.894273\n",
      "test accuracy 0.3456\n",
      "step 800, training accuracy 0.310000, loss 1.996349\n",
      "test accuracy 0.3434\n",
      "step 900, training accuracy 0.370000, loss 1.652266\n",
      "test accuracy 0.3478\n",
      "step 1000, training accuracy 0.240000, loss 1.973677\n",
      "test accuracy 0.3304\n",
      "step 1100, training accuracy 0.390000, loss 1.765133\n",
      "test accuracy 0.3294\n",
      "step 1200, training accuracy 0.400000, loss 1.619569\n",
      "test accuracy 0.3478\n",
      "step 1300, training accuracy 0.380000, loss 1.807019\n",
      "test accuracy 0.399\n",
      "step 1400, training accuracy 0.380000, loss 1.652281\n",
      "test accuracy 0.4066\n",
      "step 1500, training accuracy 0.450000, loss 1.693339\n",
      "test accuracy 0.4372\n",
      "step 1600, training accuracy 0.450000, loss 1.525174\n",
      "test accuracy 0.433\n",
      "step 1700, training accuracy 0.450000, loss 1.665701\n",
      "test accuracy 0.4426\n",
      "step 1800, training accuracy 0.440000, loss 1.613827\n",
      "test accuracy 0.4542\n",
      "step 1900, training accuracy 0.430000, loss 1.497718\n",
      "test accuracy 0.4442\n",
      "step 2000, training accuracy 0.480000, loss 1.483178\n",
      "test accuracy 0.4852\n",
      "step 2100, training accuracy 0.410000, loss 1.642241\n",
      "test accuracy 0.477\n",
      "step 2200, training accuracy 0.540000, loss 1.454421\n",
      "test accuracy 0.4602\n",
      "step 2300, training accuracy 0.450000, loss 1.630554\n",
      "test accuracy 0.5112\n",
      "step 2400, training accuracy 0.450000, loss 1.608302\n",
      "test accuracy 0.5186\n",
      "step 2500, training accuracy 0.480000, loss 1.414761\n",
      "test accuracy 0.536\n",
      "step 2600, training accuracy 0.640000, loss 1.255118\n",
      "test accuracy 0.5104\n",
      "step 2700, training accuracy 0.620000, loss 1.257211\n",
      "test accuracy 0.555\n",
      "step 2800, training accuracy 0.590000, loss 1.285095\n",
      "test accuracy 0.5352\n",
      "step 2900, training accuracy 0.580000, loss 1.277495\n",
      "test accuracy 0.5716\n",
      "step 3000, training accuracy 0.390000, loss 1.721711\n",
      "test accuracy 0.5044\n",
      "step 3100, training accuracy 0.520000, loss 1.469081\n",
      "test accuracy 0.5756\n",
      "step 3200, training accuracy 0.540000, loss 1.406266\n",
      "test accuracy 0.5664\n",
      "step 3300, training accuracy 0.620000, loss 1.232856\n",
      "test accuracy 0.592\n",
      "step 3400, training accuracy 0.510000, loss 1.485821\n",
      "test accuracy 0.5702\n",
      "step 3500, training accuracy 0.630000, loss 1.203923\n",
      "test accuracy 0.5804\n",
      "step 3600, training accuracy 0.550000, loss 1.468594\n",
      "test accuracy 0.573\n",
      "step 3700, training accuracy 0.630000, loss 1.198432\n",
      "test accuracy 0.6148\n",
      "step 3800, training accuracy 0.540000, loss 1.196415\n",
      "test accuracy 0.5522\n",
      "step 3900, training accuracy 0.460000, loss 1.571801\n",
      "test accuracy 0.5594\n",
      "step 4000, training accuracy 0.550000, loss 1.276555\n",
      "test accuracy 0.5606\n",
      "step 4100, training accuracy 0.550000, loss 1.309764\n",
      "test accuracy 0.5936\n",
      "step 4200, training accuracy 0.640000, loss 1.258608\n",
      "test accuracy 0.5944\n",
      "step 4300, training accuracy 0.570000, loss 1.288386\n",
      "test accuracy 0.6076\n",
      "step 4400, training accuracy 0.540000, loss 1.361892\n",
      "test accuracy 0.5868\n",
      "step 4500, training accuracy 0.560000, loss 1.442381\n",
      "test accuracy 0.5878\n",
      "step 4600, training accuracy 0.550000, loss 1.458354\n",
      "test accuracy 0.58\n",
      "step 4700, training accuracy 0.690000, loss 1.074513\n",
      "test accuracy 0.5974\n",
      "step 4800, training accuracy 0.590000, loss 1.389569\n",
      "test accuracy 0.6236\n",
      "step 4900, training accuracy 0.720000, loss 0.963419\n",
      "test accuracy 0.6242\n",
      "step 5000, training accuracy 0.570000, loss 1.368713\n",
      "test accuracy 0.608\n",
      "step 5100, training accuracy 0.640000, loss 1.121440\n",
      "test accuracy 0.6132\n",
      "step 5200, training accuracy 0.700000, loss 1.019885\n",
      "test accuracy 0.6362\n",
      "step 5300, training accuracy 0.620000, loss 1.353772\n",
      "test accuracy 0.635\n",
      "step 5400, training accuracy 0.620000, loss 1.138888\n",
      "test accuracy 0.6114\n",
      "step 5500, training accuracy 0.610000, loss 1.210709\n",
      "test accuracy 0.631\n",
      "step 5600, training accuracy 0.540000, loss 1.383481\n",
      "test accuracy 0.6274\n",
      "step 5700, training accuracy 0.660000, loss 1.140276\n",
      "test accuracy 0.6296\n",
      "step 5800, training accuracy 0.690000, loss 1.191441\n",
      "test accuracy 0.6264\n",
      "step 5900, training accuracy 0.700000, loss 0.971142\n",
      "test accuracy 0.642\n",
      "step 6000, training accuracy 0.560000, loss 1.216783\n",
      "test accuracy 0.6308\n",
      "step 6100, training accuracy 0.690000, loss 1.131215\n",
      "test accuracy 0.6172\n",
      "step 6200, training accuracy 0.620000, loss 1.201083\n",
      "test accuracy 0.6386\n",
      "step 6300, training accuracy 0.670000, loss 1.268476\n",
      "test accuracy 0.6394\n",
      "step 6400, training accuracy 0.530000, loss 1.562403\n",
      "test accuracy 0.568\n",
      "step 6500, training accuracy 0.520000, loss 1.464207\n",
      "test accuracy 0.6248\n",
      "step 6600, training accuracy 0.630000, loss 1.116946\n",
      "test accuracy 0.6318\n",
      "step 6700, training accuracy 0.660000, loss 1.070413\n",
      "test accuracy 0.6604\n",
      "step 6800, training accuracy 0.690000, loss 1.115563\n",
      "test accuracy 0.6366\n",
      "step 6900, training accuracy 0.690000, loss 1.107893\n",
      "test accuracy 0.6442\n",
      "step 7000, training accuracy 0.590000, loss 1.142670\n",
      "test accuracy 0.6604\n",
      "step 7100, training accuracy 0.600000, loss 1.235978\n",
      "test accuracy 0.62\n",
      "step 7200, training accuracy 0.680000, loss 1.102390\n",
      "test accuracy 0.649\n",
      "step 7300, training accuracy 0.610000, loss 1.267091\n",
      "test accuracy 0.6262\n",
      "step 7400, training accuracy 0.660000, loss 1.007861\n",
      "test accuracy 0.6448\n",
      "step 7500, training accuracy 0.630000, loss 1.125000\n",
      "test accuracy 0.6154\n",
      "step 7600, training accuracy 0.620000, loss 1.208028\n",
      "test accuracy 0.661\n",
      "step 7700, training accuracy 0.760000, loss 0.864218\n",
      "test accuracy 0.6516\n",
      "step 7800, training accuracy 0.690000, loss 1.014614\n",
      "test accuracy 0.6752\n",
      "step 7900, training accuracy 0.570000, loss 1.176965\n",
      "test accuracy 0.63\n",
      "step 8000, training accuracy 0.680000, loss 0.978992\n",
      "test accuracy 0.6684\n",
      "step 8100, training accuracy 0.690000, loss 1.097693\n",
      "test accuracy 0.6462\n",
      "step 8200, training accuracy 0.740000, loss 1.020290\n",
      "test accuracy 0.6548\n",
      "step 8300, training accuracy 0.670000, loss 1.002068\n",
      "test accuracy 0.6532\n",
      "step 8400, training accuracy 0.570000, loss 1.344707\n",
      "test accuracy 0.627\n",
      "step 8500, training accuracy 0.610000, loss 1.410763\n",
      "test accuracy 0.6466\n",
      "step 8600, training accuracy 0.660000, loss 1.204974\n",
      "test accuracy 0.639\n",
      "step 8700, training accuracy 0.700000, loss 0.952186\n",
      "test accuracy 0.6658\n",
      "step 8800, training accuracy 0.680000, loss 1.076957\n",
      "test accuracy 0.6242\n",
      "step 8900, training accuracy 0.660000, loss 1.039429\n",
      "test accuracy 0.6524\n",
      "step 9000, training accuracy 0.630000, loss 1.350112\n",
      "test accuracy 0.6566\n",
      "step 9100, training accuracy 0.770000, loss 0.785295\n",
      "test accuracy 0.6538\n",
      "step 9200, training accuracy 0.740000, loss 0.950629\n",
      "test accuracy 0.6584\n",
      "step 9300, training accuracy 0.730000, loss 0.822185\n",
      "test accuracy 0.6728\n",
      "step 9400, training accuracy 0.680000, loss 0.936818\n",
      "test accuracy 0.641\n",
      "step 9500, training accuracy 0.660000, loss 1.208944\n",
      "test accuracy 0.656\n",
      "step 9600, training accuracy 0.660000, loss 1.057607\n",
      "test accuracy 0.6122\n",
      "step 9700, training accuracy 0.640000, loss 1.295368\n",
      "test accuracy 0.644\n",
      "step 9800, training accuracy 0.710000, loss 1.019774\n",
      "test accuracy 0.6464\n",
      "step 9900, training accuracy 0.740000, loss 1.032416\n",
      "test accuracy 0.6588\n",
      "step 10000, training accuracy 0.690000, loss 1.253120\n",
      "test accuracy 0.613\n",
      "(10000, 10)\n",
      "DONE NO ERROR\n"
     ]
    }
   ],
   "source": [
    "#first conv layer\n",
    "\n",
    "\n",
    "CONV_1_DEPTH = 32\n",
    "CONV_1_SIZE = 7\n",
    "CONV_2_DEPTH = 32\n",
    "CONV_2_SIZE = 5\n",
    "CONV_3_DEPTH = 16\n",
    "CONV_3_SIZE = 5\n",
    "CONV_4_DEPTH = 16\n",
    "CONV_4_SIZE = 5\n",
    "epsilon = 1e-3\n",
    "FULLY_CONNECTED_SIZE = 1024\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3072], name=\"init_x\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"init_y\")\n",
    "\n",
    "W_conv1 = weight_variable([CONV_1_SIZE, CONV_1_SIZE, 3, CONV_1_DEPTH], name=\"conv_1_weights\")\n",
    "b_conv1 = bias_variable([CONV_1_DEPTH])\n",
    "#x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "conv_1 = conv2d(x_image, W_conv1) + b_conv1\n",
    "\n",
    "batch_mean1, batch_var1 = tf.nn.moments(conv_1,[0])\n",
    "scale1 = tf.Variable(tf.ones([CONV_1_DEPTH]))\n",
    "beta1 = tf.Variable(tf.zeros([CONV_1_DEPTH]))\n",
    "batch1 = tf.nn.batch_normalization(conv_1,batch_mean1,batch_var1,beta1,scale1,epsilon)\n",
    "\n",
    "h_conv1 = tf.nn.relu(batch1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "#second conv layer\n",
    "W_conv2 = weight_variable([CONV_2_SIZE, CONV_2_SIZE, CONV_1_DEPTH, CONV_2_DEPTH], name=\"conv_2_weights\")\n",
    "b_conv2 = bias_variable([CONV_2_DEPTH])\n",
    "conv_2 = conv2d(h_pool1, W_conv2) + b_conv2\n",
    "\n",
    "batch_mean2, batch_var2 = tf.nn.moments(conv_2,[0])\n",
    "scale2 = tf.Variable(tf.ones([CONV_2_DEPTH]))\n",
    "beta2 = tf.Variable(tf.zeros([CONV_2_DEPTH]))\n",
    "batch2 = tf.nn.batch_normalization(conv_2,batch_mean2,batch_var2,beta2,scale2,epsilon)\n",
    "\n",
    "h_conv2 = tf.nn.relu(batch2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#third conv layer\n",
    "W_conv3 = weight_variable([CONV_3_SIZE, CONV_3_SIZE, CONV_2_DEPTH, CONV_3_DEPTH], name=\"conv_3_weights\")\n",
    "b_conv3 = bias_variable([CONV_3_DEPTH])\n",
    "conv_3 = conv2d(h_pool2, W_conv3) + b_conv3\n",
    "\n",
    "batch_mean3, batch_var3 = tf.nn.moments(conv_3,[0])\n",
    "scale3 = tf.Variable(tf.ones([CONV_3_DEPTH]))\n",
    "beta3 = tf.Variable(tf.zeros([CONV_3_DEPTH]))\n",
    "batch3 = tf.nn.batch_normalization(conv_3,batch_mean3,batch_var3,beta3,scale3,epsilon)\n",
    "\n",
    "h_conv3 = tf.nn.relu(batch3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "\n",
    "#fourth conv layer\n",
    "W_conv4 = weight_variable([CONV_4_SIZE, CONV_4_SIZE, CONV_3_DEPTH, CONV_4_DEPTH], name=\"conv_4_weights\")\n",
    "b_conv4 = bias_variable([CONV_4_DEPTH])\n",
    "conv_4 = conv2d(h_pool3, W_conv4) + b_conv4\n",
    "\n",
    "batch_mean4, batch_var4 = tf.nn.moments(conv_4,[0])\n",
    "scale4 = tf.Variable(tf.ones([CONV_4_DEPTH]))\n",
    "beta4 = tf.Variable(tf.zeros([CONV_4_DEPTH]))\n",
    "batch4 = tf.nn.batch_normalization(conv_4,batch_mean4,batch_var4,beta4,scale4,epsilon)\n",
    "\n",
    "h_conv4 = tf.nn.relu(batch4)\n",
    "h_pool4 = max_pool_2x2(h_conv4)\n",
    "\n",
    "\n",
    "#dense layer\n",
    "W_fc1 = weight_variable([2 * 2 * CONV_4_DEPTH, FULLY_CONNECTED_SIZE], name=\"first_fc\")\n",
    "b_fc1 = bias_variable([FULLY_CONNECTED_SIZE])\n",
    "\n",
    "h_pool4_flat = tf.reshape(h_pool3, [-1, 2*2*CONV_4_DEPTH])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool4_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "#readout\n",
    "W_fc2 = weight_variable([FULLY_CONNECTED_SIZE, 10], name=\"last_fc\")\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "beta = 0.01\n",
    "# calculate both softmax and softmax cross entropy, one for training one for testing\n",
    "softmax = tf.nn.softmax(y_conv)\n",
    "softmax_ce = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(softmax_ce)\n",
    "\n",
    "# l2 regularization\n",
    "regularizer = tf.nn.l2_loss(W_fc2) + tf.nn.l2_loss(W_fc1)\n",
    "loss = cross_entropy + beta * regularizer\n",
    "#tf.summary.scalar(\"loss\", loss)\n",
    "#train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "train_step = tf.train.GradientDescentOptimizer(.5).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# merge all summaries into a single \"operation\" which we can execute in a session \n",
    "#summary_op = tf.merge_all_summaries()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
    "    for i in range(10001):\n",
    "        batch_x,batch_y = get_batch(X_train,y_train,batch_size=100)\n",
    "        #train_step.run(feed_dict={x: batch_x, y_: batch_y})\n",
    "        \n",
    "        train, train_accuracy, train_loss = sess.run([ train_step, accuracy,loss], feed_dict={x: batch_x, y_: batch_y})\n",
    "        \n",
    "        # write log\n",
    "        #writer.add_summary(summary, i)\n",
    "        if i % 100 == 0:\n",
    "            #train_accuracy = accuracy.eval(feed_dict={x: batch_x, y_: batch_y})\n",
    "            print('step %d, training accuracy %f, loss %f' % (i, train_accuracy, train_loss))\n",
    "            print('test accuracy %g' % accuracy.eval(feed_dict={x: X_val.T, y_: y_val.T}))\n",
    "    \n",
    "    # network trained, now make predictions on the test set\n",
    "    prediction = tf.argmax(y_conv,1)\n",
    "    preds = softmax.eval(feed_dict={x: X_test.T})\n",
    "    print(preds.shape)\n",
    "    np.save(\"hw2_ans1_\" + UNI + \".npy\",preds)\n",
    "    \n",
    "print(\"DONE NO ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"hw2_ans1_\" + UNI + \".npy\", y_conv.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
